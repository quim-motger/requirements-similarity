% Chapter 1

\chapter{State of the art: systematic review} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

In the following sections of chapter \ref{Chapter2} we introduce the details about the research method used to describe the state of the art of the similarity detection techniques in duplicated text detection.

\section{Definition of the research method}

First of all, it is necessary to defined a protocol to carry on the research. In order to avoid a vague research methodology that could lead to poor results, we aim to define a review method based on a systematic review. Using this guidance, we ensure a thorough literature review of the field of interest (i.e., similarity detection in texts) of significant scientific value, which is used in this thesis as the foundations for the main developed work.

For this purpose, it is proposed to follow the guidelines of B. Kitchenham's systematic review methodology \ref{b1}, which is focused on applying this review process in the software engineering research field. This guidelines include a series of well-defined stages and processes for both planning and conducting the review. 

Therefore, we propose to design and implement the following steps for the systematic review, based on Kitchenham's proposal.

\begin{enumerate}
\item Planning the review (see section \ref{Planning})
\begin{enumerate}
\item Identifying the need for a review
\item Specifying the review questions
\item Developing a review protocol
\end{enumerate}
\item Conducting the review (see section \ref{Conducting})
\begin{enumerate}
\item Identification of the search
\item Selection of primary studies
\item Quality assessment study
\item Data extraction \& data synthesis
\end{enumerate}
\item Reporting the review (see section \ref{Reporting})
\end{enumerate}

\section{Planning the review}
\label{Planning}

The following subsections provide a description of the three main steps of the systematic review planning. In this stage, we refine the scope of the research based on the scope of our project by establishing the general requirements of this task (i.e., what do we want to research about or where do are we going to look for the required knowledge) and all related details about how to perform this review.

\subsection{Identifying the need for a review}
\label{need}

As a first step it is necessary to justify the need for a systematic review in the similarity detection field for this master thesis. If we go back to the general objectives described at section \ref{GenObj}, we can relate this to the objective \textit{O1}:

\begin{itemize}
\item[O1.] To research the state-of-the-art of the textual similarity detection field.
\end{itemize}

Although this state-of-the-art research could be executed following alternative methods to the systematic review, we justify this approach with two reasons.

The first one is related to the requirements and specific objectives of this project. The purpose of developing and evaluating specific similarity detection algorithm implementations is to provide empirical demonstrations of the most important proposals in identifying paraphrase textual units. Furthermore, and as a contribution to the field, we aim to evaluate how this approaches behave in the requirements similarity detection field. We need to ensure that our review is thorough enough to provide a general overview of the main proposals considered as suitable for solving this problematic. This output can then be used as an input to choose specific implementations to develop and to evaluate in the scope of this master thesis.

The second one is related to the available literature in textual duplicate detection. Kitchenham's methodology states the importance of looking for any systematic reviews available on the field, which in case of existing would undermine the need of performing a systematic review for this master thesis. Therefore as a first step we focus on looking for already available state-of-the-art reviews.

We focus on looking for any review related to the similarity detection field using Natural Language Processing, Machine Learning or general Artificial Intelligence techniques. To increase the results of the research, we do not focus on systematic reviews, but in any kind of research regardless the methodology. 

The following databases are used to look for literature review of the field of study:

\begin{itemize}
\item \textbf{Scopus} \ref{Scopus}
\item \textbf{ACM Digital Library} \ref{ACM}
\item \textbf{IEEE Xplorer} \ref{IEEE}
\item \textbf{Science Direct} \ref{ScienceDirect}
\end{itemize}

Based on the target of the literature review, we propose a search string composed by three blocks of data or information we are interested in capturing with the research:

\begin{enumerate}
\item \textbf{Similarity field.} We use any match with \textit{similar*, duplicat*} or \textit{paraphras*}, which are three of the main synonyms used to refer to a pair of texts which may be considered as semantically equivalent. 
\item \textbf{Artificial Intelligence technologies.} The search must be restricted to the detection of similar textual items using \textit{AI} technologies, with a special emphasis in Natural Language and Machine Learning techniques, which represent the main approaches for this issue.
\item \textbf{State-of-the-art review.} In this stage of the systematic review, it is necessary to focus only on papers and publications which contribute providing a detailed state-of-the-art analysis. Therefore it is important to use terms such as \textit{review} or \textit{state of the art} as part of the search. This will guarantee that one of the main goals of the results in the search are focused on providing this state-of-the-art as an output of the publication.
\end{enumerate}

Consequently, the following search string is proposed:

\begin{center}
(similar*  OR  duplicat*  OR  paraphras* )  AND  ( "natural language"  OR  "machine learning"  OR  "artificial intelligence"  OR  "AI" OR  "NLP"  OR  "ML" )  AND  (review  OR  "state of the art")
\end{center}

We apply this search to the title and the keywords of the publications - this will ensure a minimum noise on the results which are not specifically focused on these three main blocks. 

These are the results:

\begin{itemize}
\item Scopus - 1 result.
\item ACM Digital Library - 1 result.
\item IEEE Xplorer- 0 results.
\item Science Direct - 0 results.
\end{itemize}

After a general overview of the results provided by Scopus and ACM, it is concluded than none of the two results are related to the paraphrase detection field in natural language texts and hence they can be excluded.
As a conclusion, it is stated that there are no publications providing a detailed, structured analysis of the state-of-the-art techniques for similarity detection between natural language text pairs. Therefore it is confirmed the need of a systematic review as a first step of this master thesis.

\subsection{Specifying the review question}

Kitchenham's methodology states the need of defining three elements of the research to help designing and defining the review scope. Applied to the software engineering field, these items are:

\begin{itemize}
\item \textbf{Population}: it refers to groups or agents (subjects) that are affected by the intervention of the review question. In this thesis, and due to the fact that natural language paraphrase detection is a wide application area, we generally focus on software Engineering teams (developers, team managers...), which are the agents interested in using this automated techniques for solving the problem addressed in this master thesis.
\item \textbf{Intervention}: it applies to the technologies used to address a specific issue. We focus on the automated similarity detection techniques and the algorithmic approaches using machine learning, natural language processing and artificial intelligence techniques in general.
\item \textbf{Outcome}: it relates to factors and output data which are relevant to evaluate the quality of a specific solution and to compare them. For evaluating the quality of the developed algorithms, we will focus on the accuracy and the performance of these solutions.
\end{itemize}

Using these three elements as an input, we focus our systematic review in solving two related research questions:

\begin{enumerate}
\item How does the software engineering community handles automated similarity detection between natural language text pairs using Artificial Intelligence (i.e., Natural Language Processing and Machine Learning)?
\item Which are the results of these general approaches in terms of accuracy and performance and which are considered as the best approaches from a qualitatively point of view using these indicators?
\end{enumerate}

\subsection{Developing a review protocol}

Once the basis of the review has been established, the next step is to define a practical review protocol to be applied when conducting the search. This synthesizes which data to look for, the filters to apply to the result data, and how to extract and analyze the information of each publication from a practical point of view.

\subsubsection{The search strategy: search \& selection procedures}

Analogously to the requirements of the systematic review need justification, we focus on answering the questions \textit{'what'} (i.e., which data are we going to look for) and \textit{'where'} (i.e., which databases are going to be used).

\begin{enumerate}
\item The selected databases are the same ones used in section \ref{need}: Scopus, ACM Digital Library, IEEE Xplorer and Science Direct
\item The search string is composed of two of the three main blocks of the one used in section \ref{need}, removing the part related to the state-of-the-art review:
\begin{center}
(similar*  OR  duplicat*  OR  paraphras* )  AND  ( "natural language"  OR  "machine learning"  OR  "artificial intelligence"  OR  "AI" OR  "NLP"  OR  "ML" )
\end{center}
\end{enumerate}

Once the required input data for the research has been selected, we can define a review protocol to look for and to filter the research results. This protocol is proposed to follow three steps:

\begin{enumerate}
\item To search in the databases using the search string defined above.
\item To merge the documents into a single repository and to remove duplicates (i.e., publications found in more than one database). For this purpose, and for future management tasks related to the literature documentation, we propose to use the Mendeley tool \ref{mendeley}, a research documentation reference manager.
\item To filter the found documents following a study selection criteria. This procedure is proposed to followed the next stages.
\begin{enumerate}
\item First, to filter publications by title, removing all results that are clearly out of the scope of our research
\item Second, to filter by reading the abstract of the publication
\item Third, to filter by giving a general overview to the document, also known as \textit{skimming}. This include reading some of the main important sections (i.e., introduction and conclusions) or by evaluating the general structure of the paper.
\item Fourth, to give a full reading to the document.
\end{enumerate}
\end{enumerate}

\subsubsection{Data extraction \& synthesis strategies}

We propose a template (see table \ref{Table1}) to fulfill for each document of the systematic review. The purpose of these template is to provide a general, structured analysis of the most relevant issues of each publication.

\begin{table}[h]
\begin{tabular}{|p{4cm}|p{10cm}|}
\hline
\textbf{Topic}                                   & \textbf{Details}                                                                                                                                                                                                              \\
\hline
Domain of the proposal                  & Is a domain-specific proposal? If it is, which domain?                                                                                                                                                               \\
\hline
Objects of similarity detection         & Which are the subjects of the similarity detection (full documents, short texts, sentences...)? Does the proposed methodology apply to a specific textual entity?                                                    \\
\hline
Similarity algorithm description        & Does it include a sequential description of the technical process? Is it detailed enough to reproduce?                                                                                                               \\
\hline
NLP preprocessing                       & Does the algorithm include a NLP preprocess step? Which are the tasks related to natural language preprocessing of the data?                                                                                         \\
\hline
Similarity functions                    & Are any word-to-word similarity functions used? If so, which ones? Does it include a specific aggregated function proposal?                                                                                          \\
\hline
Machine learning classification process & Does the algorithm include a classification process? Which kind of features are used (semantical, lexical, synctactical…)                                                                                            \\
\hline
Output results                          & Is a similarity score available for each pair of compared text items? Is there a implicit/explicit classification result (i.e. are duplicates retrieved by the algorithm itself, or a threshold score must be used?) \\
\hline
Frameworks and external tools           & Is there a list/reference to NLP/ML frameworks and third-party tools used for the similarity detection process? Are they free-to-use?                                                                                \\
\hline
Experimentation                         & Is the algorithm tested with real-data experiments? Is data available? Which kind of data is used (type, volume…)?                                                                                                   \\
\hline
Results                                 & Is there any reference to results in terms of accuracy? Reliability? Execution time? Used hardware resources?                                                                                                        \\
\hline
Evaluable tools                         & Is the proposal distributed as a tool or piece of code which can be tested?                                                                                                                                 
\\
\hline
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\section{Conducting the review}
\label{Conducting}

This sections describes the process of the conducted review and all the decisions that have been made along this process, in correlation with the previous systematic review plan. It also collects the problems and difficulties found during the systematic review process.

\subsection{Conducting the search}

This section relates to steps 1 and 2 of the review protocol: \textit{To search in the databases using the search string defined above} and \textit{To merge the documents into a single repository and to remove duplicates}.

Two main problems are faced during the search:

\begin{itemize}
\item Due to the complexity of the search string, which uses a logical combination of union and intersection logical operations between terms, it was required to adapt the search string to each consulted database. Each document search engine uses its own syntax to define searches and these logical operation between strings and hence it was necessary to adapt and test each one to guarantee the results were applying to our criteria.
\item As introduced in section \ref{Planning}, it is necessary to identify and remove duplicated results from the search in order to avoid redundancies. Sometimes publications metadata or document variations present some anomalies making it difficult to automatically detect these duplicates. However, using the previously mentioned Mendeley tool, it is also possible to detect documents which are not exact duplicates but have a high probability of being replicates. These feature was used to solve this issue.
\end{itemize}

After facing the first issue, the search is conducted, retrieving the following results.

\begin{itemize}
\item Scopus - 193 results
\item ACM Digital Library - 121 results
\item IEEE Xplorer- 38 results
\item Science Direct - 6 results
\end{itemize}

The previous list only indexes individual results for each database. The second step of the review protocol is to use the reference manager to remove duplicates and nearly duplicates, which relates to the second problem faced during this step. 

\begin{itemize}
\item \textbf{Total nº documents = 358}
\item \textbf{Duplicates = 48}
\item \textbf{Almost duplicates = 11}
\item \textbf{Final nº documents = 358 - (48 + 11) = 299}
\end{itemize}

At the end of step 2, the research has achieved a total number of 299 publications to be reviewed in the following stages.

\subsection{Selection of primary studies}

This section relates to step 3 of the review protocol: \textit{To filter the found documents following a study selection criteria.}. This phase is composed by 4 \textit{sub-steps} where each one acts as a filter to reduce the number of documents based on non-relevant research studies identification.

The first step is to \textbf{filter by the title} of the publication. In this step the number of documents is reduced from 299 to \textbf{74}, which means a total of 225 papers are deleted from the manager tool. The main reasons for discarding these documents are covered in the following list:
\begin{itemize}
\item An important number of papers are focused on the medical field - specifically, they address detecting similarities and replication patterns using artificial intelligence techniques, such as parallel patients, or disease diagnosis by medical recognition. All papers focused on different areas than natural language similarity are rejected.
\item Some of the publications are focused on grammar and syntax knowledge of non-romance languages like Hindi or Chinese. The rules and techniques of natural language are highly coupled to the main characteristics of the language itself. Therefore, we exclude these publications and focus on those using techniques for the English language.
\item Although matching the search string and non of the above restrictions, some titles suggested that some of the works were focused on solving different problems that are out of scope of this master thesis - for instance, language translation. These works are also eliminated from our article repository.
\end{itemize}

We pass all works which title does seem to focus on the research field and do not satisfy any of the restriction criteria commented above.

The second step is to \textbf{filter by the abstract} by reading it carefully and acquiring a deeper understanding on the subject that the information provided by the title, which sometimes might be vague or imprecise. In this second step we reduce the number of works from 74 to \textbf{34}, discarding a total of 40 works due to reasons like the ones listed below:
\begin{itemize}
\item Some abstracts give us a deeper knowledge on some of the filtering criteria used on the previous step, like out of field subjects (i.e., an abstract introduced that the work was focused on web-service similartiy) or the use of different languages (i.e., Turkish).
\item Some works that study the text similarity are not focused on the semantic dimension, but on other criteria such as the authorship of a piece of text. Therefore they target the study of properties and other criteria which is of no interest for the scope of this thesis
\end{itemize}

The third step is based on applying a \textbf{skimming} or general overview evaluation of the whole work, reading some of the main parts and studying the general structure and main features of its contributions. In this step we reduce the number of documents from 34 to 15. Listed below are some of the filter criteria used in this stage:
\begin{itemize}
\item Non-relevant contributions or out-of-date approaches that are better approached and discarded in other more recent works are removed. By applying this simplification we decrease redundancy and focus on evaluating actually relevant contributions.
\item Works claiming poor results or a lack of achievement of their main goals
\item Some works are focused on studying and analyzing the word-to-word similarity evaluation. Technical details and the study of word-to-word similarity are assumed to be already acknowledged and this master thesis focuses on providing solutions for the similarity detection between text pairs, specifically requirement pairs.
\item Some of the general approaches are \textbf{ontology based}, which means they require of an explicit, modeled knowledge of the domain of the input data to detect similarities. We decide to discard this kind of solutions of the scope of this thesis for two reasons: the first one is due to the fact that the data of the use case this master thesis will be validated with does not contain any kind of explicit ontology based domain knowledge; the second one is with the purpose of reducing the scope of the master thesis and therefore providing more accurate and detailed analysis on non-ontology based solutions.
\item Solutions and algorithm proposals with a clear lack of details for analyzing and reproducing the solutions by developing them are also removed.
\end{itemize}
Finally we apply a \textbf{full reading} filter of each publication. As a final result, we keep \textbf{12} of the last 15 works. The last removals are justified below:
\begin{itemize}
\item A paper was too focused on plagiarism and it was more an index of tools and frameworks than a proposal or a technical description of a similarity process.
\item A paper was too focused on a tool portfolio and although it provided results and an overview of different algorithms and tools, it did not introduce further details that were necessary to the research.
\item A paper required a first process in which training data required chunk identification. Moreover, it did not include enough detail for all features used in the feature extraction process (a total nº of 247 features).
\end{itemize}

\subsection{Data extraction and synthesis}

